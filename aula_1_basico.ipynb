{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4158a1be",
   "metadata": {},
   "source": [
    "# Aul√£o LLM's e Agentes - B√°sico de LangChain\n",
    "\n",
    "## Etapa 1 - Pergunta de Engajamento\n",
    "\n",
    "\n",
    "Construir um sistema simples que responde a perguntas sobre um t√≥pico espec√≠fico da escola.\n",
    "\n",
    "Isso √© bem simples, mas √© legal para mostrar como o LangChain organiza, estrutura e facilita o uso de LLMs com prompts reutiliz√°veis.\n",
    "\n",
    "### 1) Sem o Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed393a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui v√£o 3 op√ß√µes de perguntas iniciais, cada uma focando em uma possible principal preocupa√ß√£o do pai:\n",
      "\n",
      "- Pergunta 1 (vestibular/universidade): ‚ÄúQuais s√£o as suas principais preocupa√ß√µes em rela√ß√£o √† prepara√ß√£o do seu filho para o vestibular e para a universidade? h√° metas ou prazos espec√≠ficos que voc√™ espera que a escola ajude a alcan√ßar?‚Äù\n",
      "\n",
      "- Pergunta 2 (seguran√ßa): ‚ÄúQuais aspectos de seguran√ßa, ambiente e bem-estar s√£o mais importantes para voc√™ ao escolher o ensino m√©dio para o seu filho?‚Äù\n",
      "\n",
      "- Pergunta 3 (socializa√ß√£o/desenvolvimento): ‚ÄúQu√£o importante √© para voc√™ o aspecto social e o desenvolvimento socioemocional (conviv√™ncia, atividades extracurriculares, mentoria) no ensino m√©dio?‚Äù\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "# Inputs\n",
    "segmento = \"Ensino M√©dio\"\n",
    "\n",
    "# Prompt montado na m√£o - v√™ que aqui eu usei uma f-string \n",
    "prompt = f\"\"\"Voc√™ √© um assistente de admiss√µes de uma escola de excel√™ncia.\n",
    "O pai est√° interessado no {segmento}.\n",
    "Gere 3 op√ß√µes de perguntas iniciais para entender a principal preocupa√ß√£o dele (ex: vestibular, seguran√ßa, socializa√ß√£o).\"\"\"\n",
    "\n",
    "# com o gpt seria assim(apesar de ter v√°rias outras formas):\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70a490",
   "metadata": {},
   "source": [
    "Ou com o Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832371e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°! √â um prazer receb√™-lo(a) e discutir o futuro educacional do seu filho(a) aqui na nossa escola. Para que eu possa oferecer as informa√ß√µes mais relevantes e personalizadas, gostaria de fazer algumas perguntas iniciais para entender o que √© mais importante para voc√™ e sua fam√≠lia neste momento.\n",
      "\n",
      "Aqui est√£o 3 op√ß√µes de perguntas que poder√≠amos usar:\n",
      "\n",
      "1.  **Foco no Objetivo Geral e no Futuro:**\n",
      "    \"Bem-vindo(a)! Para come√ßarmos, adoraria entender o que os trouxe at√© n√≥s hoje. Pensando no Ensino M√©dio, qual √© o principal objetivo ou a expectativa mais importante que voc√™ tem para o seu filho(a) nesta fase, seja academicamente, socialmente ou em rela√ß√£o ao futuro?\"\n",
      "\n",
      "2.  **Foco nas Prioridades de Escolha da Escola:**\n",
      "    \"√â uma grande decis√£o escolher a escola certa para o Ensino M√©dio. Ao considerar as op√ß√µes, quais s√£o os aspectos que mais pesam na sua decis√£o por uma escola? Existe alguma √°rea espec√≠fica ‚Äì seja a prepara√ß√£o para a universidade, o desenvolvimento pessoal, a seguran√ßa ou o ambiente de aprendizagem ‚Äì que √© uma prioridade para voc√™ neste momento?\"\n",
      "\n",
      "3.  **Foco no Perfil do Aluno e Ambiente Ideal:**\n",
      "    \"Cada aluno √© √∫nico. Pensando no perfil do seu filho(a), qual tipo de ambiente escolar e quais oportunidades voc√™ acredita que seriam ideais para ele(a) prosperar plenamente no Ensino M√©dio? H√° algo espec√≠fico que voc√™ busca para que ele(a) se sinta seguro(a), desafiado(a) ou engajado(a)?\"\n",
      "\n",
      "Qualquer uma dessas op√ß√µes √© um excelente ponto de partida para nossa conversa! Estou pronto(a) para ouvir o que voc√™ tiver a compartilhar.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "# Inputs\n",
    "segmento = \"Ensino M√©dio\"\n",
    "\n",
    "# Prompt montado na m√£o - v√™ que aqui eu usei uma f-string \n",
    "prompt = f\"\"\"Voc√™ √© um assistente de admiss√µes de uma escola de excel√™ncia.\n",
    "O pai est√° interessado no {segmento}.\n",
    "Gere 3 op√ß√µes de perguntas iniciais para entender a principal preocupa√ß√£o dele (ex: vestibular, seguran√ßa, socializa√ß√£o).\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108ac17",
   "metadata": {},
   "source": [
    "### 2) Com o Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e66177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fealp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Aqui est√£o tr√™s op√ß√µes de perguntas iniciais que podem ajudar a entender as principais preocupa√ß√µes do pai em rela√ß√£o ao Ensino M√©dio:\n",
      "\n",
      "1. **Qual √© a sua principal preocupa√ß√£o em rela√ß√£o √† prepara√ß√£o do seu filho para o vestibular e o futuro acad√™mico?**\n",
      "   \n",
      "2. **Como voc√™ avalia a import√¢ncia da seguran√ßa e do ambiente escolar para o bem-estar do seu filho durante o Ensino M√©dio?**\n",
      "   \n",
      "3. **Quais s√£o suas expectativas em rela√ß√£o √† socializa√ß√£o e ao desenvolvimento de habilidades interpessoais do seu filho nesse per√≠odo de transi√ß√£o?**\n",
      "\n",
      "Essas perguntas podem ajudar a identificar as prioridades e preocupa√ß√µes do pai, permitindo uma conversa mais direcionada sobre como a escola pode atender √†s suas necessidades.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Inputs\n",
    "segmento = \"Ensino M√©dio\"\n",
    "\n",
    "# usar um template √© mais bizu\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\" Voc√™ √© um assistente de admiss√µes de uma escola de excel√™ncia.\n",
    "    O pai est√° interessado no {segmento}.\n",
    "    Gere 3 op√ß√µes de perguntas iniciais para entender a principal preocupa√ß√£o dele (ex: vestibular, seguran√ßa, socializa√ß√£o).\n",
    "    \"\"\",\n",
    "    input_variables=[\"segmento\"], # colocamos as vari√°veis de input\n",
    ")\n",
    "\n",
    "prompt = template.format(\n",
    "    segmento=segmento,\n",
    ")\n",
    "\n",
    "# gpt\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "# gemini\n",
    "#llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "# esse invoke √© bem √∫til, voc√™ pode usar tanto o \"prompt\" quanto um hist√≥rico de mensagens, etc\n",
    "resposta = llm.invoke(prompt)\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266da6e1",
   "metadata": {},
   "source": [
    "Aqui vemos a 1¬∫ facilidade do langchain, a versatilidade de usar diferentes LLM's;\n",
    "V√™ como √© complicado trocar o modelo de maneira r√°pida e dessa forma a gente s√≥ muda o par√¢metro `llm` e a sa√≠da √© a mesma.\n",
    "\n",
    "Mas temos outra utilidade, e se quisersemos obter uma sa√≠da estruturada, como um JSON?\n",
    "\n",
    "Aqui vamos ver um exemplo b√°sico primeiro, usando a sa√≠da como string mesmo para vermos como funciona o parser e a cadeia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cee35d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√°! üëã Sou o assistente de admiss√µes da escola Futuro Brilhante. Fico feliz em saber que voc√™ est√° considerando nosso Ensino M√©dio para seu filho(a)! Aqui, temos um forte compromisso com a prepara√ß√£o para as universidades federais, oferecendo um acompanhamento personalizado e recursos que fazem a diferen√ßa. \n",
      "\n",
      "Quais s√£o as principais expectativas que voc√™ tem em rela√ß√£o √† forma√ß√£o do seu filho(a) nesse per√≠odo t√£o importante?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Atue como um assistente de admiss√µes da escola 'Futuro Brilhante'.\n",
    "    \n",
    "    Sua tarefa √© escrever uma mensagem curta de 'quebra-gelo' para enviar no WhatsApp\n",
    "    para um respons√°vel interessado no segmento: {segmento}.\n",
    "    \n",
    "    Contexto da fam√≠lia:\n",
    "    - O foco principal deles parece ser: {foco_familia}.\n",
    "    - O tom da conversa deve ser: {tom_de_voz}.\n",
    "    \n",
    "    A mensagem deve terminar com uma pergunta aberta para engajar o respons√°vel.\n",
    "    \"\"\",\n",
    "    input_variables=[\"segmento\", \"foco_familia\", \"tom_de_voz\"]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Constru√ß√£o da Cadeia - template + llm + parser de sa√≠da(aqui s√≥ string mesmo)\n",
    "cadeia = template | llm | StrOutputParser()\n",
    "\n",
    "# aqui o invoke a gente pode s√≥ colocar as input variables e usar o template\n",
    "resposta = cadeia.invoke(\n",
    "    {\n",
    "        \"segmento\": \"Ensino M√©dio\",\n",
    "        \"foco_familia\": \"aprova√ß√£o em universidades federais\",\n",
    "        \"tom_de_voz\": \"confiante e profissional\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc6577",
   "metadata": {},
   "source": [
    "## Etapa 2 - Chains com OutputParser\n",
    "Agora quero tornar o analisador de atendimento da Etapa 1 estruturado e confi√°vel. Queremos que a sa√≠da da LLM n√£o seja apenas um texto solto, mas sim um JSON com os seguintes campos:\n",
    "\n",
    "- `resumo_pedido`: um resumo do que o respons√°vel deseja\n",
    "- `dados_contato`: um dicion√°rio com os dados identificados (Nome, Telefone, Email)\n",
    "- `acao_recomendada`: a pr√≥xima a√ß√£o sugerida para o atendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b610722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acao_recomendada': 'Enviar informa√ß√µes sobre valores e disponibilidade de '\n",
      "                     'vaga para o 4¬∫ ano pela WhatsApp.',\n",
      " 'dados_contato': {'Contatos': {'WhatsApp': 'sim'}, 'Nome': 'Ana'},\n",
      " 'resumo_pedido': 'A cliente Ana solicita informa√ß√µes sobre valores e '\n",
      "                  'disponibilidade de vagas para seu filho Pedro, que est√° no '\n",
      "                  '4¬∫ ano, no per√≠odo da manh√£.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pprint import pprint\n",
    "from typing import Optional\n",
    "\n",
    "# schema da resposta que definimos do Pydantic\n",
    "class AnaliseAtendimento(BaseModel):\n",
    "    resumo_pedido: str = Field(description=\"Resumo da solicita√ß√£o do respons√°vel\")\n",
    "    dados_contato: Optional[dict] = Field(default=None, description=\"Dicion√°rio com chaves Nome, Contatos\") # pode ser nulo\n",
    "    acao_recomendada: str = Field(description=\"A√ß√£o sugerida: Agendar Visita, Enviar PDF ou Ligar\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=AnaliseAtendimento)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"Analise a mensagem de atendimento recebida pelo canal \"{canal}\", referente ao segmento \"{segmento}\".\n",
    "    \n",
    "    Mensagem do Cliente: \"{mensagem}\"\n",
    "    \n",
    "    Extraia as informa√ß√µes e inclua:\n",
    "    - Um resumo do pedido\n",
    "    - Um dicion√°rio com dados de contato\n",
    "    - A a√ß√£o recomendada\n",
    "\n",
    "    {format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"canal\", \"segmento\", \"mensagem\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Cadeia: template + llm + parser(aqui o parser √© o JsonOutputParser, que vai gerar o JSON estruturado)\n",
    "cadeia = template | llm | parser\n",
    "\n",
    "resposta = cadeia.invoke({\n",
    "    \"canal\": \"WhatsApp\",\n",
    "    \"segmento\": \"Ensino Fundamental\",\n",
    "    \"mensagem\": \"Oi, sou a Ana. Meu filho Pedro est√° no 4¬∫ ano. Gostaria de saber os valores e se tem vaga de manh√£. Meu zap √© esse mesmo.\"\n",
    "})\n",
    "\n",
    "pprint(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32538f0e",
   "metadata": {},
   "source": [
    "Perceba que definimos os dados de contato como um dicion√°rio de maneira aberta (bem \"ruim\"), mas poderia ser uma classe Pydantic tamb√©m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2433e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acao_recomendada': 'Enviar informa√ß√µes sobre valores e disponibilidade de '\n",
      "                     'vagas para o 4¬∫ ano de manh√£ via WhatsApp.',\n",
      " 'dados_contato': {'contatos': {'whatsapp': 'sim'}, 'nome': 'Ana'},\n",
      " 'resumo_pedido': 'A cliente Ana deseja informa√ß√µes sobre valores e '\n",
      "                  'disponibilidade de vagas para o 4¬∫ ano de seu filho Pedro '\n",
      "                  'no per√≠odo da manh√£.'}\n"
     ]
    }
   ],
   "source": [
    "class DadosContato(BaseModel):\n",
    "    nome: Optional[str] = Field(default=None, description=\"Nome do respons√°vel\")\n",
    "    contatos: Optional[dict] = Field(default=None, description=\"Informa√ß√µes de contato (telefone, email, etc.)\")\n",
    "\n",
    "class AnaliseAtendimento(BaseModel):\n",
    "    resumo_pedido: str = Field(description=\"Resumo da solicita√ß√£o do respons√°vel\")\n",
    "    dados_contato: DadosContato = Field(description=\"Dicion√°rio com chaves Nome, Telefone e Email (se encontrados)\")\n",
    "    acao_recomendada: str = Field(description=\"A√ß√£o sugerida: Agendar Visita, Enviar PDF ou Ligar\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=AnaliseAtendimento)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"Analise a mensagem de atendimento recebida pelo canal \"{canal}\", referente ao segmento \"{segmento}\".\n",
    "    \n",
    "    Mensagem do Cliente: \"{mensagem}\"\n",
    "    \n",
    "    Extraia as informa√ß√µes e inclua:\n",
    "    - Um resumo do pedido\n",
    "    - Um dicion√°rio com dados de contato\n",
    "    - A a√ß√£o recomendada\n",
    "\n",
    "    {format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"canal\", \"segmento\", \"mensagem\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Cadeia: template + llm + parser(aqui o parser √© o JsonOutputParser, que vai gerar o JSON estruturado)\n",
    "cadeia = template | llm | parser\n",
    "\n",
    "resposta = cadeia.invoke({\n",
    "    \"canal\": \"WhatsApp\",\n",
    "    \"segmento\": \"Ensino Fundamental\",\n",
    "    \"mensagem\": \"Oi, sou a Ana. Meu filho Pedro est√° no 4¬∫ ano. Gostaria de saber os valores e se tem vaga de manh√£. Meu zap √© esse mesmo.\"\n",
    "})\n",
    "\n",
    "pprint(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e51ac5",
   "metadata": {},
   "source": [
    "Com esse exemplo que passamos, voc√™ j√° pode perceber a vantagem disso, bem al√©m de voc√™ enviar isso para um banco de dados, voc√™ pode usar isso para alimentar outras cadeias, agentes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ba688",
   "metadata": {},
   "source": [
    "## Etapa 3 ‚Äî Encadeamento de m√∫ltiplas chains\n",
    "\n",
    "Vamos criar um sistema de \"Reengajamento Automatizado\" que analisa conversas pausadas, seleciona conte√∫do relevante e gera mensagens de retomada personalizadas.\n",
    "Fluxo:\n",
    "\n",
    "1. Analisa o hist√≥rico e identifica em que fase o lead parou\n",
    "2. Cruza o diagn√≥stico com banco de materiais e escolhe o conte√∫do adequado\n",
    "3. Gera texto de retomada contextualizado de maneira personalizada\n",
    "\n",
    "Para isso, vamos usar o arquivo `marketings.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b655e",
   "metadata": {},
   "source": [
    "Primeiramente, vamos pensar na classifica√ß√£o do lead, para depois irmos para a sele√ß√£o de conte√∫do e gera√ß√£o da mensagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c3b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foco_interesse': 'M√©todos de ensino utilizados, especificamente sobre '\n",
      "                   'apostilas e livros did√°ticos.',\n",
      " 'hipotese_inatividade': 'O respons√°vel pode estar aguardando a opini√£o da '\n",
      "                         'esposa antes de prosseguir com a decis√£o.',\n",
      " 'perfil_comunicacao': 'Objetivo',\n",
      " 'resumo_conversa': 'O respons√°vel demonstrou interesse em saber como funciona '\n",
      "                    'o ensino fundamental 1, questionou sobre o uso de '\n",
      "                    'apostilas ou livros e mencionou que iria discutir com a '\n",
      "                    'esposa antes de tomar uma decis√£o.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, List\n",
    "import json\n",
    "load_dotenv()\n",
    "\n",
    "TipoPerfil = Literal[\"Objetivo\", \"Detalhista\", \"Afetivo\", \"Desconfiado\", \"Negociador\"]\n",
    "\n",
    "# √† t√≠tulo de exemplo, mostrando como usar um \"dropdown\" de poss√≠veis classifica√ß√µes para perfil_comunicacao - usar o Literal\n",
    "# vai ser mais √∫til nas tags\n",
    "class Diagnostico(BaseModel):\n",
    "    \"\"\"Diagn√≥stico de intera√ß√£o com respons√°vel.\"\"\"\n",
    "    resumo_conversa: str = Field(description=\"Resumo dos principais pontos discutidos na conversa\")\n",
    "    perfil_comunicacao: TipoPerfil = Field(description=\"Estilo de comunica√ß√£o predominante identificado no respons√°vel\")\n",
    "    hipotese_inatividade: str = Field(description=\"Principal motivo prov√°vel para a falta de resposta ou sil√™ncio\")\n",
    "    foco_interesse: str = Field(description=\"Tema ou aspecto que mais despertou interesse\")\n",
    "\n",
    "\n",
    "# Configura√ß√£o do LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "# Parsers\n",
    "parser_diag = JsonOutputParser(pydantic_object=Diagnostico)\n",
    "\n",
    "# Prompt\n",
    "# vamos colocar o hist√≥rico no prompt, mas vamos ver futuramente como melhorar essa abordagem\n",
    "prompt_diag = PromptTemplate(\n",
    "    template=\"\"\"Voc√™ √© um analista de vendas educacionais especializado em diagn√≥stico de leads.\n",
    "Sua tarefa √© analisar o hist√≥rico de conversa abaixo e extrair insights sobre o respons√°vel.\n",
    "Seja preciso, objetivo e baseie-se apenas nas informa√ß√µes fornecidas no hist√≥rico.\n",
    "\n",
    "hist√≥rico:\n",
    "{historico}\n",
    "{format_instructions}\"\"\",\n",
    "    input_variables=[\"historico\"],\n",
    "    partial_variables={\"format_instructions\": parser_diag.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "chain_diag = prompt_diag | llm | parser_diag\n",
    "\n",
    "\n",
    "historico = [\n",
    "    {\"role\": \"assistant\", \"content\": \"Ol√°! Bem-vindo √† Escola Futuro. Como posso ajudar?\"},\n",
    "    {\"role\": \"human\", \"content\": \"Oi, queria saber como funciona o ensino fundamental 1.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Claro! Focamos em autonomia e projetos. Quer agendar uma visita?\"},\n",
    "    {\"role\": \"human\", \"content\": \"Ah, legal. Mas voc√™s usam apostilas ou livros?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Usamos livros did√°ticos e projetos maker.\"},\n",
    "    {\"role\": \"human\", \"content\": \"Entendi. Vou ver com a minha esposa.\"},\n",
    "    {\"role\": \"system\", \"content\": \"Lead inativo h√° 3 dias.\"}\n",
    "]\n",
    "\n",
    "resultado = chain_diag.invoke({\n",
    "    \"historico\": json.dumps(historico, ensure_ascii=False)\n",
    "})\n",
    "\n",
    "pprint(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b8fa4",
   "metadata": {},
   "source": [
    "Ok. Vamos passar para a parte da busca.\n",
    "\n",
    "Aqui vamos passar os conte√∫dos de marketing para o prompt basicamente por did√°tica, obviamente isso n√£o escala, mas voc√™ poderia fazer v√°rias outras abordagens, vou te mostrar 2:\n",
    "\n",
    "1. Na realiza√ß√£o do diagn√≥stico, tente classificar o Lead em tags. Essas tags podem ser usadas para buscar conte√∫dos relacionados que tem as mesmas tags no banco de dados, ent√£o voc√™ faz um ranking e escolhe.\n",
    "\n",
    "2. Vamos futuramente ver RAG (Retrieval-augmented generation), onde voc√™ pode indexar esses conte√∫dos em um vetor e fazer uma busca vetorial para trazer os conte√∫dos mais relevantes. Voc√™ pode fazer uma busca por similaridade com o diagn√≥stico/resumo e trazer os conte√∫dos mais similares. Em palavras simples, voc√™ vai conseguir trazer os conte√∫dos que tem descri√ß√µes mais similares √† descri√ß√£o/resumo do lead. Essa √© a abordagem mais robusta e escal√°vel se voc√™ tem muitos conte√∫dos que s√£o similares entre si(Ex: Pela abordagem 1, tanto faz voc√™ escolher um conte√∫do que tem as tags \"Seguran√ßa\" e \"Infraestrutura\" e outro que tamb√©m tem as mesmas tags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbe36d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id_material\": \"MAT_001\",\n",
      "  \"titulo_material\": \"Comparativo: M√©todo Tradicional vs Construtivista\",\n",
      "  \"match_reasoning\": \"Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, permitindo uma an√°lise objetiva das abordagens pedag√≥gicas da escola.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DecisaoConteudo(BaseModel):\n",
    "    id_material: str = Field(description=\"O ID do material escolhido da lista.\")\n",
    "    titulo_material: str = Field(description=\"O t√≠tulo do material escolhido.\")\n",
    "    match_reasoning: str = Field(description=\"Explica√ß√£o breve do motivo deste conte√∫do √© adequado para este perfil de respons√°vel.\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "parser = JsonOutputParser(pydantic_object=DecisaoConteudo)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Atue como um Especialista de Marketing Escolar.\n",
    "    \n",
    "    Sua miss√£o √© escolher o MELHOR material de apoio para enviar a um lead, baseando-se no perfil dele.\n",
    "    \n",
    "    Perfil do Lead:\n",
    "    - Estilo de Comunica√ß√£o: {perfil_comunicacao}\n",
    "    - Foco de Interesse: {foco_interesse}\n",
    "    - Hipotese para Inatividade: {hipotese_inatividade}\n",
    "\n",
    "    Conte√∫dos dispon√≠veis:\n",
    "    {banco_conteudos}\n",
    "    \n",
    "    Regras:\n",
    "    1. Escolha apenas 1 material que tenha o maior \"Match\" com o interesse do lead.\n",
    "    2. Retorne o ID exato.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"perfil_comunicacao\", \"foco_interesse\", \"hipotese_inatividade\", \"banco_conteudos\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | llm | parser\n",
    "\n",
    "\n",
    "# Exemplo: Vamos colocar obtidos anteriormente, por exemplo     \n",
    "perfil_comunicacao = \"Objetivo\"\n",
    "foco_interesse = \"M√©todos de ensino e materiais utilizados (livros did√°ticos e projetos maker).\"\n",
    "hipotese_inatividade = \"O respons√°vel pode estar aguardando a opini√£o da esposa antes de tomar uma decis√£o.\"\n",
    "\n",
    "# Carregando o banco de conte√∫dos de marketing\n",
    "banco_conteudos = json.load(open(\"marketings.json\", \"r\", encoding=\"utf-8\"))\n",
    "# passando para string\n",
    "# nesse caso passamos tudo que estava no banco, mas voc√™ poderia passar somente as descri√ß√µes e os IDs\n",
    "banco_conteudos = json.dumps(banco_conteudos, ensure_ascii=False)\n",
    "\n",
    "resultado = chain.invoke({\n",
    "    \"perfil_comunicacao\": perfil_comunicacao,\n",
    "    \"foco_interesse\": foco_interesse,\n",
    "    \"hipotese_inatividade\": hipotese_inatividade,\n",
    "    \"banco_conteudos\": banco_conteudos\n",
    "})\n",
    "\n",
    "print(json.dumps(resultado, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164b27e",
   "metadata": {},
   "source": [
    "Massa! Vamos ver agora a parte da mensagem final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013fd600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texto': 'Ol√°! Espero que voc√™ esteja bem. Estou retomando o contato para compartilhar um material que acredito ser do seu interesse: um comparativo entre o M√©todo Tradicional e o Construtivista. Esse conte√∫do oferece uma vis√£o clara sobre as diferentes abordagens pedag√≥gicas, que podem ser muito √∫teis para suas decis√µes. Se precisar de mais informa√ß√µes ou quiser discutir o assunto, estou √† disposi√ß√£o!'}\n"
     ]
    }
   ],
   "source": [
    "class MensagemResgate(BaseModel):\n",
    "    texto: str = Field(description=\"Texto da mensagem de resgate para o WhatsApp\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7) # vamos colocar uma temperatura maior para ser mais criativo e natural\n",
    "parser = JsonOutputParser(pydantic_object=MensagemResgate)\n",
    "\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Atue como um Resgatador de Lead por meio de uma mensagem de WhatsApp.\n",
    "    Escreva uma mensagem de retomada de contato juntamente com o conte√∫do de marketing disponibilizado.\n",
    "    O conte√∫do vai ser enviado junto com a mensagem, ent√£o n√£o repita informa√ß√µes do material.\n",
    "\n",
    "    Conte√∫do:\n",
    "    - T√≠tulo: {titulo_material}\n",
    "    - Motivo para a escolha do conte√∫do: {match_reasoning}\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"perfil\", \"interesse\", \"motivo_pausa\", \"titulo_material\", \"match_reasoning\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | llm | parser\n",
    "\n",
    "# vamos supor que os resultados da chain anterior foram:\n",
    "titulo_material = \"Comparativo: M√©todo Tradicional vs Construtivista\"\n",
    "match_reasoning = \"Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, proporcionando uma compara√ß√£o clara entre abordagens pedag√≥gicas.\"\n",
    "\n",
    "mensagem = chain.invoke({\n",
    "    \"titulo_material\": titulo_material,\n",
    "    \"match_reasoning\": match_reasoning\n",
    "})\n",
    "\n",
    "print(mensagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc53e64",
   "metadata": {},
   "source": [
    "Beleza, mas como eu fa√ßo esse processo todo de maneira encadeada? Ou seja, quero que a sa√≠da de uma etapa seja a entrada da pr√≥xima. Vou reescrever o c√≥digo todo aqui para ficar mais claro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834528a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"historico\": [\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Ol√°! Bem-vindo √† Escola Futuro. Como posso ajudar?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"human\",\n",
      "      \"content\": \"Oi, queria saber como funciona o ensino fundamental 1.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Claro! Focamos em autonomia e projetos. Quer agendar uma visita?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"human\",\n",
      "      \"content\": \"Ah, legal. Mas voc√™s usam apostilas ou livros?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Usamos livros did√°ticos e projetos maker.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"human\",\n",
      "      \"content\": \"Entendi. Vou ver com a minha esposa.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Lead inativo h√° 3 dias.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"historico\": [\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Ol√°! Bem-vindo √† Escola Futuro. Como posso ajudar?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"human\",\n",
      "      \"content\": \"Oi, queria saber como funciona o ensino fundamental 1.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Claro! Focamos em autonomia e projetos. Quer agendar uma visita?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"human\",\n",
      "      \"content\": \"Ah, legal. Mas voc√™s usam apostilas ou livros?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Usamos livros did√°ticos e projetos maker.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"human\",\n",
      "      \"content\": \"Entendi. Vou ver com a minha esposa.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Lead inativo h√° 3 dias.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Voc√™ √© um analista de vendas educacionais especializado em diagn√≥stico de leads.\\nSua tarefa √© analisar o hist√≥rico de conversa abaixo e extrair insights sobre o respons√°vel.\\nSeja preciso, objetivo e baseie-se apenas nas informa√ß√µes fornecidas no hist√≥rico.\\n\\nhist√≥rico:\\n[{'role': 'assistant', 'content': 'Ol√°! Bem-vindo √† Escola Futuro. Como posso ajudar?'}, {'role': 'human', 'content': 'Oi, queria saber como funciona o ensino fundamental 1.'}, {'role': 'assistant', 'content': 'Claro! Focamos em autonomia e projetos. Quer agendar uma visita?'}, {'role': 'human', 'content': 'Ah, legal. Mas voc√™s usam apostilas ou livros?'}, {'role': 'assistant', 'content': 'Usamos livros did√°ticos e projetos maker.'}, {'role': 'human', 'content': 'Entendi. Vou ver com a minha esposa.'}, {'role': 'system', 'content': 'Lead inativo h√° 3 dias.'}]\\nSTRICT OUTPUT FORMAT:\\n- Return only the JSON value that conforms to the schema. Do not include any additional text, explanations, headings, or separators.\\n- Do not wrap the JSON in Markdown or code fences (no ``` or ```json).\\n- Do not prepend or append any text (e.g., do not write \\\"Here is the JSON:\\\").\\n- The response must be a single top-level JSON value exactly as required by the schema (object/array/etc.), with no trailing commas or comments.\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]} the object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema (shown in a code block for readability only ‚Äî do not include any backticks or Markdown in your output):\\n```\\n{\\\"description\\\": \\\"Diagn√≥stico de intera√ß√£o com respons√°vel.\\\", \\\"properties\\\": {\\\"resumo_conversa\\\": {\\\"description\\\": \\\"Resumo dos principais pontos discutidos na conversa\\\", \\\"title\\\": \\\"Resumo Conversa\\\", \\\"type\\\": \\\"string\\\"}, \\\"perfil_comunicacao\\\": {\\\"description\\\": \\\"Estilo de comunica√ß√£o predominante identificado no respons√°vel\\\", \\\"enum\\\": [\\\"Objetivo\\\", \\\"Detalhista\\\", \\\"Afetivo\\\", \\\"Desconfiado\\\", \\\"Negociador\\\"], \\\"title\\\": \\\"Perfil Comunicacao\\\", \\\"type\\\": \\\"string\\\"}, \\\"hipotese_inatividade\\\": {\\\"description\\\": \\\"Principal motivo prov√°vel para a falta de resposta ou sil√™ncio\\\", \\\"title\\\": \\\"Hipotese Inatividade\\\", \\\"type\\\": \\\"string\\\"}, \\\"foco_interesse\\\": {\\\"description\\\": \\\"Tema ou aspecto que mais despertou interesse\\\", \\\"title\\\": \\\"Foco Interesse\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"resumo_conversa\\\", \\\"perfil_comunicacao\\\", \\\"hipotese_inatividade\\\", \\\"foco_interesse\\\"]}\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [3.76s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"resumo_conversa\\\":\\\"O respons√°vel demonstrou interesse no funcionamento do ensino fundamental 1, questionou sobre o uso de apostilas ou livros e mencionou que iria consultar a esposa antes de tomar uma decis√£o.\\\",\\\"perfil_comunicacao\\\":\\\"Objetivo\\\",\\\"hipotese_inatividade\\\":\\\"O respons√°vel pode estar aguardando a opini√£o da esposa antes de prosseguir com a decis√£o.\\\",\\\"foco_interesse\\\":\\\"M√©todos de ensino utilizados, especificamente sobre apostilas e livros did√°ticos.\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"resumo_conversa\\\":\\\"O respons√°vel demonstrou interesse no funcionamento do ensino fundamental 1, questionou sobre o uso de apostilas ou livros e mencionou que iria consultar a esposa antes de tomar uma decis√£o.\\\",\\\"perfil_comunicacao\\\":\\\"Objetivo\\\",\\\"hipotese_inatividade\\\":\\\"O respons√°vel pode estar aguardando a opini√£o da esposa antes de prosseguir com a decis√£o.\\\",\\\"foco_interesse\\\":\\\"M√©todos de ensino utilizados, especificamente sobre apostilas e livros did√°ticos.\\\"}\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 96,\n",
      "                \"prompt_tokens\": 668,\n",
      "                \"total_tokens\": 764,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_provider\": \"openai\",\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_11f3029f6b\",\n",
      "              \"id\": \"chatcmpl-Cn8ERoHaMrllk6t1LbanZemh6QgBw\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"lc_run--019b236a-a546-7d11-94cb-18c8026fbb62-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 668,\n",
      "              \"output_tokens\": 96,\n",
      "              \"total_tokens\": 764,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 96,\n",
      "      \"prompt_tokens\": 668,\n",
      "      \"total_tokens\": 764,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_provider\": \"openai\",\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_11f3029f6b\",\n",
      "    \"id\": \"chatcmpl-Cn8ERoHaMrllk6t1LbanZemh6QgBw\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"resumo_conversa\": \"O respons√°vel demonstrou interesse no funcionamento do ensino fundamental 1, questionou sobre o uso de apostilas ou livros e mencionou que iria consultar a esposa antes de tomar uma decis√£o.\",\n",
      "  \"perfil_comunicacao\": \"Objetivo\",\n",
      "  \"hipotese_inatividade\": \"O respons√°vel pode estar aguardando a opini√£o da esposa antes de prosseguir com a decis√£o.\",\n",
      "  \"foco_interesse\": \"M√©todos de ensino utilizados, especificamente sobre apostilas e livros did√°ticos.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"resumo_conversa\": \"O respons√°vel demonstrou interesse no funcionamento do ensino fundamental 1, questionou sobre o uso de apostilas ou livros e mencionou que iria consultar a esposa antes de tomar uma decis√£o.\",\n",
      "  \"perfil_comunicacao\": \"Objetivo\",\n",
      "  \"hipotese_inatividade\": \"O respons√°vel pode estar aguardando a opini√£o da esposa antes de prosseguir com a decis√£o.\",\n",
      "  \"foco_interesse\": \"M√©todos de ensino utilizados, especificamente sobre apostilas e livros did√°ticos.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    Atue como um Especialista de Marketing Escolar.\\n\\n    Sua miss√£o √© escolher o MELHOR material de apoio para enviar a um lead, baseando-se no perfil dele.\\n\\n    Perfil do Lead:\\n    - Estilo de Comunica√ß√£o: Objetivo\\n    - Foco de Interesse: M√©todos de ensino utilizados, especificamente sobre apostilas e livros did√°ticos.\\n    - Hipotese para Inatividade: O respons√°vel pode estar aguardando a opini√£o da esposa antes de prosseguir com a decis√£o.\\n\\n    Conte√∫dos dispon√≠veis:\\n    [{\\\"id\\\": \\\"MAT_001\\\", \\\"titulo\\\": \\\"Comparativo: M√©todo Tradicional vs Construtivista\\\", \\\"tags\\\": [\\\"pedag√≥gico\\\", \\\"metodologia\\\"], \\\"formato\\\": \\\"Infogr√°fico PDF\\\"}, {\\\"id\\\": \\\"MAT_002\\\", \\\"titulo\\\": \\\"Calculadora de Investimento e Descontos\\\", \\\"tags\\\": [\\\"financeiro\\\", \\\"pre√ßo\\\", \\\"bolsa\\\"], \\\"formato\\\": \\\"Link Interativo\\\"}, {\\\"id\\\": \\\"MAT_003\\\", \\\"titulo\\\": \\\"V√≠deo: Um dia na vida dos alunos\\\", \\\"tags\\\": [\\\"rotina\\\", \\\"ambiente\\\", \\\"seguran√ßa\\\"], \\\"formato\\\": \\\"V√≠deo Youtube\\\"}, {\\\"id\\\": \\\"MAT_004\\\", \\\"titulo\\\": \\\"Resultados no Vestibular 2024\\\", \\\"tags\\\": [\\\"performance\\\", \\\"resultado\\\"], \\\"formato\\\": \\\"PDF\\\"}]\\n\\n    Regras:\\n    1. Escolha apenas 1 material que tenha o maior \\\"Match\\\" com o interesse do lead.\\n    2. Retorne o ID exato.\\n\\n    STRICT OUTPUT FORMAT:\\n- Return only the JSON value that conforms to the schema. Do not include any additional text, explanations, headings, or separators.\\n- Do not wrap the JSON in Markdown or code fences (no ``` or ```json).\\n- Do not prepend or append any text (e.g., do not write \\\"Here is the JSON:\\\").\\n- The response must be a single top-level JSON value exactly as required by the schema (object/array/etc.), with no trailing commas or comments.\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]} the object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema (shown in a code block for readability only ‚Äî do not include any backticks or Markdown in your output):\\n```\\n{\\\"properties\\\": {\\\"id_material\\\": {\\\"description\\\": \\\"O ID do material escolhido da lista.\\\", \\\"title\\\": \\\"Id Material\\\", \\\"type\\\": \\\"string\\\"}, \\\"titulo_material\\\": {\\\"description\\\": \\\"O t√≠tulo do material escolhido.\\\", \\\"title\\\": \\\"Titulo Material\\\", \\\"type\\\": \\\"string\\\"}, \\\"match_reasoning\\\": {\\\"description\\\": \\\"Explica√ß√£o breve do motivo deste conte√∫do √© adequado para este perfil de respons√°vel.\\\", \\\"title\\\": \\\"Match Reasoning\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"id_material\\\", \\\"titulo_material\\\", \\\"match_reasoning\\\"]}\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.31s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"id_material\\\":\\\"MAT_001\\\",\\\"titulo_material\\\":\\\"Comparativo: M√©todo Tradicional vs Construtivista\\\",\\\"match_reasoning\\\":\\\"Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, permitindo uma an√°lise objetiva das apostilas e livros did√°ticos utilizados.\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"id_material\\\":\\\"MAT_001\\\",\\\"titulo_material\\\":\\\"Comparativo: M√©todo Tradicional vs Construtivista\\\",\\\"match_reasoning\\\":\\\"Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, permitindo uma an√°lise objetiva das apostilas e livros did√°ticos utilizados.\\\"}\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 60,\n",
      "                \"prompt_tokens\": 687,\n",
      "                \"total_tokens\": 747,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_provider\": \"openai\",\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_11f3029f6b\",\n",
      "              \"id\": \"chatcmpl-Cn8EUKhKAPhtb1hEiWAkCSJ85oawd\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"lc_run--019b236a-b3fc-74b3-b535-e17e6614cb94-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 687,\n",
      "              \"output_tokens\": 60,\n",
      "              \"total_tokens\": 747,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 60,\n",
      "      \"prompt_tokens\": 687,\n",
      "      \"total_tokens\": 747,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_provider\": \"openai\",\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_11f3029f6b\",\n",
      "    \"id\": \"chatcmpl-Cn8EUKhKAPhtb1hEiWAkCSJ85oawd\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"id_material\": \"MAT_001\",\n",
      "  \"titulo_material\": \"Comparativo: M√©todo Tradicional vs Construtivista\",\n",
      "  \"match_reasoning\": \"Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, permitindo uma an√°lise objetiva das apostilas e livros did√°ticos utilizados.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"id_material\": \"MAT_001\",\n",
      "  \"titulo_material\": \"Comparativo: M√©todo Tradicional vs Construtivista\",\n",
      "  \"match_reasoning\": \"Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, permitindo uma an√°lise objetiva das apostilas e livros did√°ticos utilizados.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    Atue como um Resgatador de Lead por meio de uma mensagem de WhatsApp.\\n    Escreva uma mensagem de retomada de contato juntamente com o conte√∫do de marketing disponibilizado.\\n    O conte√∫do vai ser enviado junto com a mensagem, ent√£o n√£o repita informa√ß√µes do material.\\n\\n    Conte√∫do:\\n    - T√≠tulo: Comparativo: M√©todo Tradicional vs Construtivista\\n    - Motivo para a escolha do conte√∫do: Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, permitindo uma an√°lise objetiva das apostilas e livros did√°ticos utilizados.\\n\\n    STRICT OUTPUT FORMAT:\\n- Return only the JSON value that conforms to the schema. Do not include any additional text, explanations, headings, or separators.\\n- Do not wrap the JSON in Markdown or code fences (no ``` or ```json).\\n- Do not prepend or append any text (e.g., do not write \\\"Here is the JSON:\\\").\\n- The response must be a single top-level JSON value exactly as required by the schema (object/array/etc.), with no trailing commas or comments.\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]} the object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema (shown in a code block for readability only ‚Äî do not include any backticks or Markdown in your output):\\n```\\n{\\\"properties\\\": {\\\"texto\\\": {\\\"description\\\": \\\"Texto da mensagem de resgate para o WhatsApp\\\", \\\"title\\\": \\\"Texto\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"texto\\\"]}\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [3.20s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"texto\\\":\\\"Ol√°! Espero que voc√™ esteja bem. Queremos retomar nosso contato e compartilhar um material que pode ser muito √∫til para voc√™. Preparamos um comparativo entre o M√©todo Tradicional e o Construtivista, que aborda diretamente os m√©todos de ensino. Acreditamos que esse conte√∫do ser√° relevante para a sua an√°lise das apostilas e livros did√°ticos. N√£o hesite em nos dar um retorno!\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"texto\\\":\\\"Ol√°! Espero que voc√™ esteja bem. Queremos retomar nosso contato e compartilhar um material que pode ser muito √∫til para voc√™. Preparamos um comparativo entre o M√©todo Tradicional e o Construtivista, que aborda diretamente os m√©todos de ensino. Acreditamos que esse conte√∫do ser√° relevante para a sua an√°lise das apostilas e livros did√°ticos. N√£o hesite em nos dar um retorno!\\\"}\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 85,\n",
      "                \"prompt_tokens\": 413,\n",
      "                \"total_tokens\": 498,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_provider\": \"openai\",\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_ee69c2ef48\",\n",
      "              \"id\": \"chatcmpl-Cn8EWElhH4ewDU13kr1i3vnv989Vn\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"lc_run--019b236a-b919-7661-9305-8cf7110a890b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 413,\n",
      "              \"output_tokens\": 85,\n",
      "              \"total_tokens\": 498,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 85,\n",
      "      \"prompt_tokens\": 413,\n",
      "      \"total_tokens\": 498,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_provider\": \"openai\",\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_ee69c2ef48\",\n",
      "    \"id\": \"chatcmpl-Cn8EWElhH4ewDU13kr1i3vnv989Vn\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"texto\": \"Ol√°! Espero que voc√™ esteja bem. Queremos retomar nosso contato e compartilhar um material que pode ser muito √∫til para voc√™. Preparamos um comparativo entre o M√©todo Tradicional e o Construtivista, que aborda diretamente os m√©todos de ensino. Acreditamos que esse conte√∫do ser√° relevante para a sua an√°lise das apostilas e livros did√°ticos. N√£o hesite em nos dar um retorno!\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [8.28s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"texto\": \"Ol√°! Espero que voc√™ esteja bem. Queremos retomar nosso contato e compartilhar um material que pode ser muito √∫til para voc√™. Preparamos um comparativo entre o M√©todo Tradicional e o Construtivista, que aborda diretamente os m√©todos de ensino. Acreditamos que esse conte√∫do ser√° relevante para a sua an√°lise das apostilas e livros did√°ticos. N√£o hesite em nos dar um retorno!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# para ver o processo ocorrendo por baixo dos panos, vamos colocar o set_debug\n",
    "\n",
    "from langchain_core.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "# Carregando o banco de conte√∫dos de marketing\n",
    "banco_conteudos = json.load(open(\"marketings.json\", \"r\", encoding=\"utf-8\"))\n",
    "# passando para string\n",
    "# nesse caso passamos tudo que estava no banco, mas voc√™ poderia passar somente as descri√ß√µes e os IDs\n",
    "banco_conteudos = json.dumps(banco_conteudos, ensure_ascii=False)\n",
    "\n",
    "TipoPerfil = Literal[\"Objetivo\", \"Detalhista\", \"Afetivo\", \"Desconfiado\", \"Negociador\"]\n",
    "\n",
    "class Diagnostico(BaseModel):\n",
    "    \"\"\"Diagn√≥stico de intera√ß√£o com respons√°vel.\"\"\"\n",
    "    resumo_conversa: str = Field(description=\"Resumo dos principais pontos discutidos na conversa\")\n",
    "    perfil_comunicacao: TipoPerfil = Field(description=\"Estilo de comunica√ß√£o predominante identificado no respons√°vel\")\n",
    "    hipotese_inatividade: str = Field(description=\"Principal motivo prov√°vel para a falta de resposta ou sil√™ncio\")\n",
    "    foco_interesse: str = Field(description=\"Tema ou aspecto que mais despertou interesse\")\n",
    "\n",
    "class DecisaoConteudo(BaseModel):\n",
    "    id_material: str = Field(description=\"O ID do material escolhido da lista.\")\n",
    "    titulo_material: str = Field(description=\"O t√≠tulo do material escolhido.\")\n",
    "    match_reasoning: str = Field(description=\"Explica√ß√£o breve do motivo deste conte√∫do √© adequado para este perfil de respons√°vel.\")\n",
    "\n",
    "class MensagemResgate(BaseModel):\n",
    "    texto: str = Field(description=\"Texto da mensagem de resgate para o WhatsApp\")\n",
    "\n",
    "# llm's\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "llm_resgate = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7) # mais criativo para resgate\n",
    "\n",
    "# parsers\n",
    "parser_diag = JsonOutputParser(pydantic_object=Diagnostico)\n",
    "parser_conteudo = JsonOutputParser(pydantic_object=DecisaoConteudo)\n",
    "parser_resgate = JsonOutputParser(pydantic_object=MensagemResgate)\n",
    "\n",
    "# templates\n",
    "template_diag = PromptTemplate(\n",
    "    template=\"\"\"Voc√™ √© um analista de vendas educacionais especializado em diagn√≥stico de leads.\n",
    "Sua tarefa √© analisar o hist√≥rico de conversa abaixo e extrair insights sobre o respons√°vel.\n",
    "Seja preciso, objetivo e baseie-se apenas nas informa√ß√µes fornecidas no hist√≥rico.\n",
    "\n",
    "hist√≥rico:\n",
    "{historico}\n",
    "{format_instructions}\"\"\",\n",
    "    input_variables=[\"historico\"],\n",
    "    partial_variables={\"format_instructions\": parser_diag.get_format_instructions()}\n",
    ")\n",
    "\n",
    "template_conteudo = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Atue como um Especialista de Marketing Escolar.\n",
    "    \n",
    "    Sua miss√£o √© escolher o MELHOR material de apoio para enviar a um lead, baseando-se no perfil dele.\n",
    "    \n",
    "    Perfil do Lead:\n",
    "    - Estilo de Comunica√ß√£o: {perfil_comunicacao}\n",
    "    - Foco de Interesse: {foco_interesse}\n",
    "    - Hipotese para Inatividade: {hipotese_inatividade}\n",
    "\n",
    "    Conte√∫dos dispon√≠veis:\n",
    "    {banco_conteudos}\n",
    "    \n",
    "    Regras:\n",
    "    1. Escolha apenas 1 material que tenha o maior \\\"Match\\\" com o interesse do lead.\n",
    "    2. Retorne o ID exato.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"perfil_comunicacao\", \"foco_interesse\", \"hipotese_inatividade\", \"banco_conteudos\"],\n",
    "    partial_variables={\n",
    "        \"banco_conteudos\": banco_conteudos,\n",
    "        \"format_instructions\": parser_conteudo.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "template_resgate = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Atue como um Resgatador de Lead por meio de uma mensagem de WhatsApp.\n",
    "    Escreva uma mensagem de retomada de contato juntamente com o conte√∫do de marketing disponibilizado.\n",
    "    O conte√∫do vai ser enviado junto com a mensagem, ent√£o n√£o repita informa√ß√µes do material.\n",
    "\n",
    "    Conte√∫do:\n",
    "    - T√≠tulo: {titulo_material}\n",
    "    - Motivo para a escolha do conte√∫do: {match_reasoning}\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"perfil_comunicacao\", \"foco_interesse\", \"hipotese_inatividade\", \"titulo_material\", \"match_reasoning\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# chains\n",
    "chain_diag = template_diag | llm | parser_diag\n",
    "chain_conteudo = template_conteudo | llm | parser_conteudo\n",
    "chain_resgate = template_resgate | llm_resgate | parser_resgate\n",
    "# principal agora;\n",
    "chain_principal = chain_diag | chain_conteudo | chain_resgate\n",
    "\n",
    "historico = [\n",
    "    {\"role\": \"assistant\", \"content\": \"Ol√°! Bem-vindo √† Escola Futuro. Como posso ajudar?\"},\n",
    "    {\"role\": \"human\", \"content\": \"Oi, queria saber como funciona o ensino fundamental 1.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Claro! Focamos em autonomia e projetos. Quer agendar uma visita?\"},\n",
    "    {\"role\": \"human\", \"content\": \"Ah, legal. Mas voc√™s usam apostilas ou livros?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Usamos livros did√°ticos e projetos maker.\"},\n",
    "    {\"role\": \"human\", \"content\": \"Entendi. Vou ver com a minha esposa.\"},\n",
    "    {\"role\": \"system\", \"content\": \"Lead inativo h√° 3 dias.\"}\n",
    "]\n",
    "\n",
    "mensagem = chain_principal.invoke({\n",
    "    \"historico\": historico,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170f7d8",
   "metadata": {},
   "source": [
    "`Output Final`:\n",
    "\n",
    "_Ol√°! Tudo bem? Espero que voc√™ esteja tendo um √≥timo dia! Eu gostaria de retomar nosso contato e compartilhar um material interessante que aborda o comparativo entre o M√©todo Tradicional e o Construtivista. Acredito que voc√™ achar√° √∫til para entender melhor as abordagens pedag√≥gicas que estamos discutindo. Estou √† disposi√ß√£o para conversar mais sobre o assunto!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c309a9",
   "metadata": {},
   "source": [
    "_Por que n√£o fazer de maneira unificada? Sla, um √∫nico prompt que fa√ßa tudo?_ \n",
    "\n",
    "Desse jeito voc√™ consegue trabalhar de maneira mais individual em cada etapa, testar, validar, ajustar, etc. O que √© mais valioso √© que voc√™ consegue separar as etapas de pensamento da IA e n√£o fazer com que ela pense do jeito dela. Voc√™ tem bem mais controle de tudo que t√° acontecendo. Al√©m disso, voc√™ pode reutilizar essas cadeias em outros fluxos tamb√©m. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1abbef",
   "metadata": {},
   "source": [
    "**Observa√ß√£o**: no pipe puro `A | B | C`, voc√™ pode \"perder\" campos do estado(as vari√°veis), que evidentemente podem ser importantes.\n",
    "Aqui a gente usa `RunnablePassthrough.assign(...)` para **preservar** diagn√≥stico + decis√£o de conte√∫do at√© o final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa615189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagn√≥stico: \n",
      "\n",
      " {'resumo_conversa': 'O respons√°vel demonstrou interesse em saber como funciona o ensino fundamental 1, questionou sobre o uso de apostilas ou livros e mencionou que iria discutir a decis√£o com a esposa.', 'perfil_comunicacao': 'Objetivo', 'hipotese_inatividade': 'O respons√°vel est√° consultando a esposa antes de tomar uma decis√£o.', 'foco_interesse': 'M√©todos de ensino e materiais utilizados (apostilas ou livros)'}\n",
      "\n",
      "conte√∫do escolhido: \n",
      "\n",
      " {'id_material': 'MAT_001', 'titulo_material': 'Comparativo: M√©todo Tradicional vs Construtivista', 'match_reasoning': 'Este material aborda diretamente os m√©todos de ensino, que √© o foco de interesse do lead, permitindo uma an√°lise objetiva entre diferentes abordagens pedag√≥gicas.'}\n",
      "\n",
      "mensagem de resgate: \n",
      "\n",
      " {'texto': 'Ol√°! Espero que voc√™ esteja bem. Estou entrando em contato para ver como andam suas reflex√µes sobre m√©todos de ensino. Para te ajudar nessa an√°lise, estou compartilhando um material que fizemos sobre o comparativo entre o M√©todo Tradicional e o Construtivista. Acredito que voc√™ encontrar√° informa√ß√µes valiosas para suas decis√µes. Fico √† disposi√ß√£o para conversarmos mais sobre isso!'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "# vou colocar false porque j√° sugou\n",
    "set_debug(False)\n",
    "\n",
    "# podiamos melhorar isso assim:\n",
    "chain_principal = (\n",
    "    # diagn√≥stico (gera: resumo_conversa, perfil_comunicacao, hipotese_inatividade, foco_interesse)\n",
    "    RunnablePassthrough.assign(diagnostico=chain_diag)\n",
    "    # ‚Äúespalha‚Äù os campos do diagn√≥stico no topo (pra alimentar o pr√≥ximo prompt)\n",
    "    | RunnableLambda(lambda x: {**x, **x[\"diagnostico\"]})\n",
    "    # escolhe conte√∫do (gera: id_material, titulo_material, match_reasoning)\n",
    "    | RunnablePassthrough.assign(conteudo=chain_conteudo)\n",
    "    # espalha campos do conte√∫do no topo (pra alimentar resgate)\n",
    "    | RunnableLambda(lambda x: {**x, **x[\"conteudo\"]})\n",
    "    # mensagem final\n",
    "    | RunnablePassthrough.assign(mensagem_resgate=chain_resgate)\n",
    ")\n",
    "\n",
    "resposta = chain_principal.invoke({\"historico\": historico})\n",
    "\n",
    "# O que interessa no final:\n",
    "\n",
    "print(f'diagn√≥stico: \\n\\n {resposta[\"diagnostico\"]}')\n",
    "print(f'\\nconte√∫do escolhido: \\n\\n {resposta[\"conteudo\"]}')\n",
    "print(f'\\nmensagem de resgate: \\n\\n {resposta[\"mensagem_resgate\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13742c28",
   "metadata": {},
   "source": [
    "Boa! Finalizamos essa parte inicial que d√° pra fazer bastante coisa que enriquecem bastante o produto, seja fazendo uma automa√ß√£o para resumir informa√ß√µes do Lead, ferramentas de reengajamento, avalia√ß√£o de reuni√µes de vendas.\n",
    "\n",
    "Na aula que vem vamos ver como trabalhar com RAG e mem√≥ria para enriquecer ainda mais nossos sistemas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
